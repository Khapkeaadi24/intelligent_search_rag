# ğŸ§  Intelligent Search-Based RAG System (Hybrid Verified Mode)

An **offline, enterprise-style research assistant** that converts open-ended user queries into **structured, transparent, and confidence-aware research reports** using a Retrieval-Augmented Generation (RAG) pipeline.

This project addresses the limitations of traditional web search such as unstructured results, repetition, and lack of verification.

---

## ğŸš€ Problem Statement

Traditional web search engines return large volumes of:
- unstructured information
- repetitive content
- mixed reliability sources  

Users must manually verify, compare, and structure information, which is **time-consuming and error-prone**.

---

## âœ… Solution

This project implements an **Intelligent Search-Based RAG System** that:

- Accepts **any research query**
- Retrieves **verified contextual information**
- Separates **verified facts** from **analytical inference**
- Produces **structured research reports**
- Provides a **research confidence score**
- Exports results as **PDF reports**
- Runs **fully offline** using a local LLM

---

## ğŸ§  Key Concept: Hybrid Verified Mode

Instead of behaving like a chatbot, the system operates in **Hybrid Verified Mode**:

- âœ” **Verified Findings**  
  Information grounded in retrieved context

- âš  **Analytical Inference (Not Directly Verified)**  
  Clearly labeled reasoning based on general domain knowledge

This approach balances **accuracy, transparency, and practical usefulness**, which is critical for enterprise and research use cases.

---

## ğŸ—ï¸ System Architecture

User Query
â†“
Verified Search Layer
â†“
Text Chunking
â†“
Embeddings Generation
â†“
FAISS Vector Store
â†“
Hybrid Verified QA (Local LLM)
â†“
Structured Research Report
â†“
Confidence Score + PDF Export


---

## ğŸ§° Tech Stack

- **Python 3.11**
- **Streamlit** â€“ Web UI
- **LangChain** â€“ RAG orchestration
- **FAISS** â€“ Vector similarity search
- **SentenceTransformers** â€“ Local embeddings
- **Ollama** â€“ Local LLM runtime
- **TinyLLaMA (1.1B)** â€“ Lightweight CPU-only LLM
- **ReportLab** â€“ PDF generation

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Install Python
Install **Python 3.11.x**  
ğŸ‘‰ https://www.python.org/downloads/

> Make sure **â€œAdd Python to PATHâ€** is checked during installation.

---

### 2ï¸âƒ£ Install Ollama
Download Ollama for Windows:  
ğŸ‘‰ https://ollama.com/download

Application Preview
<img width="1916" height="904" alt="image" src="https://github.com/user-attachments/assets/2c843b83-9ab3-48ce-9cf0-4ad232e51c4c" />

After installation, pull the required model:
```bash
ollama pull tinyllama:1.1b-chat

3ï¸âƒ£ Clone or Download the Project
git clone <your-repo-url>
cd intelligent-search-rag


(or download ZIP and extract)

4ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate

5ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

6ï¸âƒ£ Run the Application
python -m streamlit run app.py


The app will open at:

http://localhost:8501

ğŸ“Š Features

ğŸ” Intelligent research for any query

ğŸ§  Hybrid Verified RAG (no hidden hallucination)

â³ Time-based filtering (6 months / 1 year)

ğŸ†š Competitor analysis mode

ğŸ” Research confidence indicator

ğŸ“„ PDF export (manager-ready reports)

ğŸ’» Fully offline execution

ğŸ“Œ Example Use Cases

Company & competitor research

Technology trend analysis

Academic or industry research

Strategic decision support

Internal knowledge exploration

âš ï¸ Limitations

Output quality depends on the quality of retrieved sources

Lightweight model used for low-resource environments

Real-time web citations are not enabled (architecture supports future integration)

ğŸ”® Future Enhancements

Live web search integration (Bing / Wikipedia)

Real URL-based citations

Domain-specific research modes

Docker-based deployment


Section-level confidence visualization


