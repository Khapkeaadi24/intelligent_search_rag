# ğŸ§  VeriSearch AI â€” Document-First Verified Research System (v2)

VeriSearch AI is an **offline, enterprise-style research assistant** that transforms user queries and uploaded documents into **structured, transparent, and confidence-aware research reports** using a **document-first Retrieval-Augmented Generation (RAG)** pipeline.

This version focuses on **professional research synthesis**, not chatbot-style answering.

---

## ğŸš© Problem Statement

Traditional research workflows suffer from:

- Unstructured search results
- Repetitive or shallow summaries
- Mixed reliability of sources
- Lack of transparency between facts and inference

Managers and researchers need **clear, auditable, and structured reports**, not chat responses.

---

## âœ… Solution Overview

VeriSearch AI provides:

- ğŸ“„ Document-first research (PDFs, text inputs)
- ğŸ§  Hybrid reasoning (verified content + analytical synthesis)
- ğŸ“Š Structured, manager-ready research reports
- ğŸ” Research confidence indicators
- ğŸ“„ Professional PDF export
- ğŸ’» Fully offline execution using a local LLM

---

## ğŸ§  Core Concept: Hybrid Verified Mode

Instead of claiming full verification, the system **clearly separates**:

### âœ” Document-Grounded Analysis
Insights derived directly from uploaded documents

### âš  Analytical Reasoning
Logical synthesis based on local LLM reasoning  
*(clearly labeled â€” no false verification claims)*

This ensures **honesty, transparency, and enterprise trust**.

---

## ğŸ—ï¸ System Architecture

User Query / Document Upload
â†“
Document Extraction
â†“
Chunking & Embeddings
â†“
Vector Retrieval (FAISS)
â†“
Local LLM Reasoning (TinyLLaMA via Ollama)
â†“
Structured Research Report
â†“
Confidence Scoring + PDF Export


---

## ğŸ§° Tech Stack

- **Python 3.11**
- **Streamlit** â€” UI
- **LangChain** â€” RAG orchestration
- **FAISS** â€” Vector similarity search
- **SentenceTransformers** â€” Local embeddings
- **Ollama** â€” Local LLM runtime
- **TinyLLaMA (1.1B)** â€” CPU-friendly LLM
- **ReportLab** â€” Professional PDF generation

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Install Python
Download Python 3.11  
ğŸ‘‰ https://www.python.org/downloads/

(Enable **Add Python to PATH**)

---

### 2ï¸âƒ£ Install Ollama
ğŸ‘‰ https://ollama.com/download

Pull the model:
```bash
ollama pull tinyllama:1.1b-chat
3ï¸âƒ£ Clone Repository
git clone https://github.com/Khapkeaadi24/intelligent_search_rag_24.git
cd intelligent-search-rag
4ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate
5ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
6ï¸âƒ£ Run Application
streamlit run app.py
Open:

http://localhost:8501
ğŸ“Š Features
ğŸ” Research on any topic

ğŸ“„ Document-based analysis

ğŸ§  Hybrid verified reasoning

â³ Time range configuration

ğŸ†š Competitor analysis framework

ğŸ“ˆ Research confidence scoring

ğŸ“„ Professional PDF export

ğŸ’» Fully offline execution

ğŸ“Œ Use Cases
Company & market research

Technology trend analysis

Academic paper review

Strategic decision support

Internal knowledge synthesis

âš ï¸ Current Limitations
No live web search or URLs (by design)

Output quality depends on uploaded documents

Lightweight LLM used for local demo purposes

ğŸ”® Planned Enhancements
Live web verification APIs

URL-level citations

Voice & image input

Multi-document cross-analysis

Docker deployment

ğŸ§¾ Methodological Note
This system does not claim real-time verification in its current version.
All results are transparently labeled and generated using local AI reasoning over user-provided documents.

